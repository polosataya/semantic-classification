# streamlit run –ê–Ω–∞–ª–∏–∑_–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.py

import streamlit as st
import pandas as pd
import numpy as np
import aspose.words as aw
import re
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import TrainingArguments, Trainer
#from guess_language import guess_language
#import pytesseract
#from pdf2image import convert_from_path


st.set_page_config(page_title="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'",
                   page_icon="üìù", layout="wide", initial_sidebar_state="expanded",
                   menu_items={'Get Help': None, 'Report a bug': None, 'About': None})

hide_streamlit_style = """<style>#MainMenu {visibility: hidden;}footer {visibility: hidden;}</style>"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

############################################################################
# –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
############################################################################

#–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
doc_type = ['docx', 'doc', 'rtf', 'pdf']

labels = ['act', 'application', 'arrangement', 'bill', 'contract', 'contract offer', 'determination', 'invoice', 'order', 'proxy', 'statute']
labels_ru = ['–ê–∫—Ç', '–ó–∞—è–≤–ª–µ–Ω–∏–µ', '–°–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–°—á–µ—Ç', '–î–æ–≥–æ–≤–æ—Ä', '–î–æ–≥–æ–≤–æ—Ä –æ—Ñ–µ—Ä—Ç—ã', '–†–µ—à–µ–Ω–∏–µ', '–°—á—ë—Ç-—Ñ–∞–∫—Ç—É—Ä–∞', '–ü—Ä–∏–∫–∞–∑', '–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–£—Å—Ç–∞–≤']

id2label = {idx:label for idx, label in enumerate(labels)}
label2id = {label:idx for idx, label in enumerate(labels)}
id2ru = {idx:label for idx, label in enumerate(labels_ru)}

#–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∑–∞–Ω–∏—è
threshold = 0.4

############################################################################
# —Ä–∞–±–æ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º
############################################################################

@st.cache_data
def load_file(filename):
    '''–°—á–∏—Ç—ã–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∑–∞–≥—Ä—É–∑–∫–∏.
    –ù–∞ –≤—Ö–æ–¥–µ:
       filename - –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å pdf, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç—Å—è –∫–∞–∫ —Ä—É—Å—Å–∫–∏–π - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ pytesseract,
        –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å–≤–µ—Ç, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "".
    –ù–∞ –≤—ã—Ö–æ–¥–µ:
       text - —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞'''
    
    text = None

    if filename.name.rsplit('.', 1)[-1] in doc_type:
        parsed = aw.Document(filename).get_text()
        text = aw_clean(parsed)
        
    else:
        #–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø
        pass

    #if filename.name.rsplit('.', 1)[-1] == 'pdf':
        #—É pdf –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –º—É—Å–æ—Ä - —Ä–∞—Å–ø–æ–∑–Ω–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    #    if guess_language(text) != "ru":
    #        text = ''
    #        pages = convert_from_path(filename, 500)
    #        for i, page in enumerate(pages):
    #            parsed = pytesseract.image_to_string(page, lang="rus")
    #            text += parsed
    #        text = aw_clean(text, [])
    #    else:
    #        pass

    return text

def aw_clean(s):
    #–ø—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    replace_dict={"–°–∫–∞—á–∞–Ω–æ —Å":" ", '–û–±—Ä–∞–∑–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞':' ', 
                 '–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω —Å–∞–π—Ç–æ–º\x13 HYPERLINK "https://dogovor-obrazets.ru"':' ',
                 '\x14https://dogovor-obrazets.ru\x15':' ',
                 '–ò—Å—Ç–æ—á–Ω–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º:\x13 HYPERLINK "https://dogovor-obrazets.ru':' ',
                 '/√ê¬¥√ê¬æ√ê¬≥√ê¬æ√ê¬≤√ê¬æ√ë\x80/√ê\x9e√ê¬±√ë\x80√ê¬∞√ê¬∑√ê¬µ√ë\x86_√ê\x94√ê¬æ√ê¬≥√ê¬æ√ê¬≤√ê¬æ√ë\x80_√ê¬ø√ê¬æ√ë\x81√ë':' ',
                  '\x82√ê¬∞√ê¬≤√ê¬∫√ê¬∏_√ë\x82√ê¬æ√ê¬≤√ê¬∞√ë\x80√ê¬∞-1" \x14https://dogovor-obrazets.ru/–¥–æ–≥–æ–≤–æ—Ä/':' ',
                  "\* MERGEFORMAT": " ", "FILLIN": " ",
                 'Evaluation Only. Created with Aspose.Words. Copyright 2003-2023 Aspose Pty Ltd.': " ",
                 'Created with an evaluation copy of Aspose.Words.': " ",
                 'To discover the full versions of our APIs please visit: https://products.aspose.com/words/':" ",
                 "This document was truncated here because it was created in the Evaluation Mode.": " ", 
                 }
    for key, value in replace_dict.items():
        s = s.replace(key, value)
    s = re.sub(r"https?://[^,\s]+,?", " ", s) #—É–¥–∞–ª–µ–Ω–∏–µ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–æ–∫
    s = re.sub('"consultantplus://offline/ref=[0-9A-Z]*"', '', s)
    s = re.sub(r'HYPERLINK \\l "Par[0-9]*"', '', s)
    s = re.sub(r'HYPERLINK', '', s)
    replace_dict={" .": ".", " ,": ",", " ¬´ ¬ª": "", "¬´ ¬ª ": "", " ¬´ ¬ª": "",' " "': "", " ‚Äú ‚Äù": "",  
                  "_":" ", "ÔøΩ":" ", "¬∑": "", "--":"", "‚Ä¶":"", "/":"", "|":"", '‚Äú‚Äù':'', "¬Æ": " ", "\d": " ", 
                  '\x0e':" ", "\x02":"", "\x0c":" ", "\x07":" ", "\xa0":" ", "\x13":" ", "\x14":" ", "\x15":" "} 
    for key, value in replace_dict.items():
        s = s.replace(key, value) 
    s = re.sub(r"\s+", " ", s).strip() #—É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏, –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ \r\n\t
    replace_dict={" ¬´ ¬ª":"", " ‚Äú ‚Äù":"", "( ) ": ""}
    for key, value in replace_dict.items():
        s = s.replace(key, value) 
    return s

############################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
############################################################################

tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-sentence')

save_directory = 'model'
best_model = BertForSequenceClassification.from_pretrained(save_directory, local_files_only=True)

trainer = Trainer(best_model)

def predict_text(text):
    #–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    encoding = tokenizer(text, return_tensors="pt", max_length=512)
    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

    outputs = trainer.model(**encoding)
    logits = outputs.logits
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(logits.squeeze().to("cpu"))
    scores = max(probs).item()
    predictions = np.zeros(probs.shape)
    predictions[np.where(probs == max(probs))] = 1  #–æ–¥–∏–Ω –ª–µ–π–±–ª
    # –ø—Ä–µ–¥–∏–∫—Ç –≤ –ª–µ–π–±–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–∞ —ç–∫—Ä–∞–Ω
    #predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
    predicted_labels = [id2ru[idx] for idx, label in enumerate(predictions) if label == 1.0][0]
    return predicted_labels, round(scores, 2)

############################################################################
# –æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å
############################################################################

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–µ–º—Å—è –±–ª–æ–∫–µ
def show_contract_text(text):
    with st.expander("–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞"):
        st.write(text)

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
upload_button = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏", type=doc_type)

if upload_button is not None:
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –∫–Ω–æ–ø–∫–∞ –Ω–∞–∂–∞—Ç–∞
    text = load_file(upload_button)
 
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    predict, scores = predict_text(text)
    if scores > threshold:
        st.write(f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {predict}")
        st.write(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {scores}")
    else:
        st.write(f"–í–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –¥–æ–ø—É—Å—Ç–∏–º–æ–º—É —Ç–∏–ø—É")
        st.write(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {scores}")

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    show_contract_text(text)
else:
    pass


